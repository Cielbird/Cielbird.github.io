<!doctype html><html class="dark light" lang=en><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://cielbird.github.io name=base><title>
            
                Distributed AI in Rust
            
        </title><meta content="Distributed AI in Rust" property=og:title><link href=https://cielbird.github.io/fonts.css rel=stylesheet><link media="(prefers-color-scheme: dark)" href=/syntax-theme-dark.css rel=stylesheet><link media="(prefers-color-scheme: light)" href=/syntax-theme-light.css rel=stylesheet><script defer src=https://cielbird.github.io/js/codeblock.js></script><script defer src=https://cielbird.github.io/js/toc.js></script><script defer src=https://cielbird.github.io/js/note.js></script><script>MathJax = {
                    tex: {
                        inlineMath: [
                            ['$', '$'],
                            ['\\(', '\\)']
                        ]
                    }
                };</script><script async id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link title="
    cielbird
" href=https://cielbird.github.io/atom.xml rel=alternate type=application/atom+xml><link href=https://cielbird.github.io/theme/light.css rel=stylesheet><link href=https://cielbird.github.io/theme/dark.css id=darkModeStyle rel=stylesheet><script src=https://cielbird.github.io/js/themetoggle.js></script><script>setTheme(getSavedTheme());</script><link href=https://cielbird.github.io/main.css media=screen rel=stylesheet><script src="https://cielbird.github.io/search_index.en.js?h=4a07d0cfaf17a5a49bae" defer></script><script src="https://cielbird.github.io/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><body><div class=left-content></div><div class=content><nav><div class=left-nav><a class=logo href=https://cielbird.github.io> <img alt=cielbird src=https://cielbird.github.io/logo.jpg> </a><div class=socials><a rel="'me'" class=social href=https://www.linkedin.com/in/guilhemane/> <img alt=linkedin src=https://cielbird.github.io/icons/social/linkedin.svg> </a><a rel="'me'" class=social href=https://github.com/cielbird/> <img alt=github src=https://cielbird.github.io/icons/social/github.svg> </a><a rel="'me'" class=social href=mailto:guilhem.ane3@gmail.com> <img alt=email src=https://cielbird.github.io/icons/social/email.svg> </a></div></div><div class=right-nav><a href=https://cielbird.github.io/blog style=margin-right:.5em>/blog</a><a href=https://cielbird.github.io/projects style=margin-right:.5em>/projects</a><a href=https://cielbird.github.io/about style=margin-right:.5em>/about</a><a href=https://cielbird.github.io/tags style=margin-right:.5em>/tags</a><button title="$SHORTCUT to open search" class=search-button id=search-button><img alt=Search class=search-icon src=https://cielbird.github.io/icons/search.svg></button><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><div id=modal-content><h1 class=page-header id=modalTitle>Search</h1><div id=searchBar><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search... role=combobox spellcheck=false><button title="Clear search" class=clear-button id=clear-search><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></button></div><div id=results-container><div id=results-info><span id=zero_results style=display:none>No results</span><span id=one_result style=display:none>1 result</span><span id=many_results style=display:none>$NUMBER results</span></div><div id=results role=listbox></div></div></div></div><a onclick="toggleTheme(); event.preventDefault();" href=# id=dark-mode-toggle> <img alt=Light id=sun-icon src=https://cielbird.github.io/icons/sun.svg style=filter:invert()> <img alt=Dark id=moon-icon src=https://cielbird.github.io/icons/moon.svg> </a><script>updateItemToggleTheme()</script></div></nav><div data-selector="main article p" class=visible-element-observer-root><main><article><div class=title><div class=page-header>Distributed AI in Rust</div><div class=meta>Posted on <time>2025-09-01</time> :: Updated on <time>2025-10-25</time><span class=tags-label>::</span><span class=tags> <a class=post-tag href=https://cielbird.github.io/tags/ai/>ai</a> <a class=post-tag href=https://cielbird.github.io/tags/rust/>rust</a> </span></div></div><section class=body><div class=note-container><div class=note-header><div class=note-icon><p>Note</div></div><div class=note-content><p>This post was originally written at the end of an internship. I haven't gotten to re-writing it for my blog, so it may read a little weird.</div></div><h1 id=introduction><a aria-label="Anchor link for: introduction" class=zola-anchor href=#introduction>Introduction</a></h1><p>AI models and their training data are big. Training an AI model on only one GPU can become slow, and many models don't even fit on one single GPU. Many techniques exist to manage the memory and speed limitations of modern models. These techniques have unlocked many of the impressive advances in machine learning we see today. These training and inference techniques all depend on basic building blocks of collective communications called <em>collective operations</em>.<p>In this blog we will cover collective operations, how they are used to speed up training. We'll also discuss the unique way <a href=https://burn.dev/>Burn</a> implements collective operations in the 0.20.0 release, as well as how you can easily train your models on multiple devices and multiple nodes.<h1 id=data-parallel-training><a aria-label="Anchor link for: data-parallel-training" class=zola-anchor href=#data-parallel-training>Data Parallel Training</a></h1><p>Distributed data parallel training (DDP) is a basic example of distributed training. It consists of splitting the training data between multiple devices and training a copy of the model on each device, all while keeping the parameters in sync.<p>First, the training data is split between each device. Then, during each training step, each device does a forward and backward pass on its own batch of data. The resulting gradients are aggregated between each device. Each device then optimizes its model with the new gradients. At this point the models have the same optimized parameters.</p><img alt="Distributed Data Parallel" aspect-ratio="1556 / 1556" decoding=async loading=lazy src=https://cielbird.github.io/processed_images/ddp.0998265185a51c66.avif><p><em>Figure 1: Data distributed training on three devices</em><p>This technique allows training time to be cut down significantly, as long as the gradient syncing is negligible. This technique still requires each device to store the entire model in memory, which is an issue tackled by other distributed training techniques.<p>It is clear that the key to this technique is the gradient syncing. The gradient syncing must be as efficient as possible as to not be a bottleneck in the pipeline.<div class=footnote-definition id=1><sup class=footnote-definition-label>1</sup><p>The term 'devices' refers to GPUs, TPUs, and other computational hardware commonly utilized in machine learning applications.</div><h1 id=collective-operations><a aria-label="Anchor link for: collective-operations" class=zola-anchor href=#collective-operations>Collective operations</a></h1><p>The syncing of gradients in a data parallel training is a collecive operation called an <em>all-reduce</em>. An all-reduce is one of many primitive collective operations. Some others are:<ul><li>broadcast: one device sends a tensor to all others<li>reduce: a tensor on each device is reduced to one tensor on one device<li>reduce-scatter: a tensor on each device is reduced, each device ends up with a part of the resulting tensor</ul><p>PyTorch and TensorFlow don't implement their own collective operations, instead they make use of communication libraries such as <a href=https://developer.nvidia.com/nccl>NCCL</a> (for NVIDIA GPUs), MPI, or Gloo.<p>NCCL is the library used for NVIDIA GPUs. It abstracts collective operations using protocols like NVLink, PCIe, GPUDirect RDMA, and even TCP/IP. For all of NCCL's benefits, it is only useful for NVIDIA devices, which goes against Burn's core principles.<p>Moreover, Burn <em>already has</em> the tools for tensor communication between devices on the same machine with <code>Tensor::to_device</code>. We can take advantage of shared memory, or even backend specific protocols like <code>NVLink</code> for an Nvidia backend. Logically, GPU-to-GPU communication on the same machine should be done with <code>to_device</code>.<p>For these reasons, we decided to implement our own collective operations crate called <code>burn-collective</code>. For intra-node communication, we use <code>Tensor::to_device</code>, taking advantage of all the backend specific optimisations. For inter-node communication, we use TCP/IP. This two-step separation will show up later.</p><img aspect-ratio="937 / 937" alt=Stack decoding=async loading=lazy src=https://cielbird.github.io/processed_images/stack.36ae8199b25fb227.avif><p><em>Figure 2: Pytorch and NCCL compared to Burn</em><h1 id=how-burn-implements-collective-operations><a aria-label="Anchor link for: how-burn-implements-collective-operations" class=zola-anchor href=#how-burn-implements-collective-operations>How Burn implements collective operations</a></h1><p>Burn currently only supports all-reduce. Reduce and broadcast are also supported, although only in single-node contexts.<h2 id=how-many-processes><a aria-label="Anchor link for: how-many-processes" class=zola-anchor href=#how-many-processes>How many processes?</a></h2><p>We started with <code>all-reduce</code>, because it is the backbone to data distributed training.<p>With PyTorch, you usually assign a different process to each GPU. There are many reasons for this, but a big one is the Python's Global Interpreter Lock (GIL). The GIL only allows one thread to hold the Python interpreter at a time, which essentially prevents anything written in Python to actually be multi-threaded.<p>Thankfully, we're not using Python.<p>As said before, we can use <code>to_device</code> to take care of intra-node communication. We can assume the user will launch a thread for each GPU. So for one machine, we only need one process.<p><img alt="Burn collective" aspect-ratio="2311 / 2311" decoding=async loading=lazy src=https://cielbird.github.io/processed_images/burn_collective.5c143b6ca24982da.avif> <em>Figure 3: Burn collective: an example structure with 4 peers and 2 nodes</em><h2 id=local-and-global><a aria-label="Anchor link for: local-and-global" class=zola-anchor href=#local-and-global>Local and global</a></h2><p>Since intra-node and inter-node communication are fundamentally different, we decided to split collective operations between a <em>local</em> (intra-node) and <em>global</em> (inter-node) level. Internally, the algorithms are implemented differently on the internal level and global levels.<p>This leads to a process-per-node structure.<p>It is worth noting that the local/global separation is an implementation detail, and it is only necessary to know when configuring the collective. From a user's perspective, all the other peers, whether on the same node or not, are just as accessible.<h2 id=walkthrough-of-an-all-reduce><a aria-label="Anchor link for: walkthrough-of-an-all-reduce" class=zola-anchor href=#walkthrough-of-an-all-reduce>Walkthrough of an All-Reduce</a></h2><p>Lets walk through the internals of a call to <code>all_reduce</code><p>Each thread must first register, passing a <code>CollectiveConfig</code> that contains information about the number of peers on the same node, as well as the number of nodes in the collective. The call to <code>register</code> is blocking, so it syncs all the threads. When the node's <code>LocalCollectiveServer</code> has registered each peer on the node, the node will register on the global level if necessary.<p>Then, on the global level, the <code>GlobalOrchestrator</code> acts as a rendez-vous point for each node. After registering, the nodes have the addresses of every other node in the collective, and they can be as independent as possible. In the future, the <code>GlobalOrchestrator</code> could allow for a dynamic topology, keeping nodes updated on any changes.<p>Next, all peers in the collective call an <code>all_reduce</code>.<p>When all registered threads have called the opration, the <code>LocalCollectiveServer</code> starts the operation. In single node contexts, this is very simple, as the <code>LocalCollectiveServer</code> manages everything with <code>Tensor::to_device</code> for tensor transfers.<p>In a multi-node context, each node will already have the coordinates of other nodes, supplied upon registering. They communicate tensors with the <code>burn_communications</code> crate, specifically with the <code>TensorDataService</code>. This service allows for exposing and downloading Burn tensors over the network in a peer-to-peer manner. Currently we use WebSockets, but QUIC is a likely candidate for future use.<p>In multi-node contexts, nodes must synchronise at the end of the operation. This is true for all collective operations, but it becomes especially important for <code>broadcast</code>, where the broadcaster must wait for all receivers to receive the tensor.<h2 id=methods><a aria-label="Anchor link for: methods" class=zola-anchor href=#methods>Methods</a></h2><p>Burn supports multiple strategies for all-reduce, configurable at both local and global levels.<h3 id=centralized><a aria-label="Anchor link for: centralized" class=zola-anchor href=#centralized>Centralized</a></h3><p>All peers send tensors to a root, which aggregates them and broadcasts the result back.<h3 id=tree><a aria-label="Anchor link for: tree" class=zola-anchor href=#tree>Tree</a></h3><p>Peers are arranged in a b-tree and reduce in parallel, achieving $O(\log_b(N))$ time.<h3 id=ring><a aria-label="Anchor link for: ring" class=zola-anchor href=#ring>Ring</a></h3><p>Peers form a ring, slicing tensors and passing them around. This maximizes bandwidth usage but is more sensitive to latency.</p><img alt="all-reduce methods" aspect-ratio="2257 / 2257" decoding=async loading=lazy src=https://cielbird.github.io/processed_images/methods.283a5a87a3fbfd0c.avif><p><em>Figure 4: An overview of the three strategies</em><h3 id=local-strategy-and-global-strategy><a aria-label="Anchor link for: local-strategy-and-global-strategy" class=zola-anchor href=#local-strategy-and-global-strategy>Local strategy and global strategy</a></h3><p>Since the all-reduce is split on two levels, the local (intra-node) level and global (inter-node) level, we can use different local strategies for different nodes, and a different strategy on the global level. Below is a diagram that shows an example of a collective with 3 nodes, each using a different local strategy.</p><img aspect-ratio="1056 / 1056" alt=all-reduce-methods-local-global decoding=async loading=lazy src=https://cielbird.github.io/processed_images/method_local_global.41503ef57a68cbc0.avif><h3 id=local-ring-downfall><a aria-label="Anchor link for: local-ring-downfall" class=zola-anchor href=#local-ring-downfall>Local ring downfall</a></h3><p>The <code>Centralized</code> and <code>Tree</code> strategies can be split into two operations: a reduce and a broadcast. A reduce operation aggregates all tensors onto one peer, and a broadcast distributes a tensor from one peer to all others.<p>The result from the global all-reduce needs to be broadcast to all other local peers. So, with <code>Ring</code> and <code>Centralized</code> we don't actually need to do a local all-reduce, we just need to do a reduce, followed by the global all-reduce, followed by the broadcast. It's like fitting the global all-reduce in the middle of the local all-reduce<p>So with <code>Centralized</code> and <code>Tree</code> in multi-node contexts we do:<p>Local reduce -> Global all-reduce -> Local broadcast<p>Due to the nature of the <code>Ring</code> algorithm, a ring-reduce can't be split between a reduce step and a broadcast step. This means if the <code>Ring</code> strategy is chosen locally, the steps will be as follows:<p>Local all-reduce -> Global all-reduce -> Local broadcast<p>This unnecessarily distributes the local all-reduce result to local peers, when anyway we will overwrite the tensor with the global all-reduce result. This may be less advantageous than other configurations. For this reason, it is recommended not to use <code>Ring</code> on the local level, only on the global level.<h1 id=burn-communications><a aria-label="Anchor link for: burn-communications" class=zola-anchor href=#burn-communications><code>burn-communications</code></a></h1><p>With the addition of <code>burn-collective</code>, it was necessary to build a solid base for network communication in burn. The <code>burn-communications</code> crate offers an abstraction of client-server logic, as well as a <code>TensorDataService</code> used for peer-to-peer tensor transfers. This allows developers to swap protocols with minimal effort.<h1 id=ddp-training><a aria-label="Anchor link for: ddp-training" class=zola-anchor href=#ddp-training>DDP Training</a></h1><p>Lets get back to a Data Parallel training. How can you take advantage of these fancy new collective operations?<p>Previously, to train on multiple devices, you had to use the <code>LearnerBuilder::devices</code> function:<pre class="language-rust z-code" data-lang=rust><code class=language-rust data-lang=rust><span class="z-source z-rust"><span class="z-storage z-type z-rust">let</span> learner <span class="z-keyword z-operator z-assignment z-rust">=</span> <span class="z-meta z-path z-rust">LearnerBuilder<span class="z-punctuation z-accessor z-rust">::</span></span>new<span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span><span class="z-constant z-other z-rust">ARTIFACT_DIR</span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span>
</span><span class="z-source z-rust">    <span class="z-punctuation z-accessor z-dot z-rust">.</span><span class="z-support z-function z-rust">devices</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span><span class="z-support z-macro z-rust">vec!</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">[</span>gpu_1<span class="z-punctuation z-separator z-rust">,</span> gpu_2<span class="z-punctuation z-separator z-rust">,</span> gpu_3<span class="z-punctuation z-section z-group z-end z-rust">]</span></span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span>
</span><span class="z-source z-rust">    <span class="z-comment z-line z-double-slash z-rust"><span class="z-punctuation z-definition z-comment z-rust">//</span> ...
</span></span><span class="z-source z-rust">    <span class="z-punctuation z-accessor z-dot z-rust">.</span><span class="z-support z-function z-rust">build</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span>model<span class="z-punctuation z-separator z-rust">,</span> config<span class="z-punctuation z-accessor z-dot z-rust">.</span>optimizer<span class="z-punctuation z-accessor z-dot z-rust">.</span><span class="z-support z-function z-rust">init</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span><span class="z-punctuation z-separator z-rust">,</span> 1e<span class="z-keyword z-operator z-arithmetic z-rust">-</span><span class="z-constant z-numeric z-integer z-decimal z-rust">4</span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span><span class="z-punctuation z-terminator z-rust">;</span>
</span></code></pre><p>This has been replaced with <code>LearnerBuilder::learning_strategy</code>:<pre class="language-rust z-code" data-lang=rust><code class=language-rust data-lang=rust><span class="z-source z-rust"><span class="z-storage z-type z-rust">let</span> collective <span class="z-keyword z-operator z-assignment z-rust">=</span> <span class="z-meta z-path z-rust">CollectiveConfig<span class="z-punctuation z-accessor z-rust">::</span></span>default<span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span><span class="z-punctuation z-terminator z-rust">;</span>
</span><span class="z-source z-rust">
</span><span class="z-source z-rust"><span class="z-storage z-type z-rust">let</span> learner <span class="z-keyword z-operator z-assignment z-rust">=</span> <span class="z-meta z-path z-rust">LearnerBuilder<span class="z-punctuation z-accessor z-rust">::</span></span>new<span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span><span class="z-constant z-other z-rust">ARTIFACT_DIR</span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span>
</span><span class="z-source z-rust">    <span class="z-punctuation z-accessor z-dot z-rust">.</span><span class="z-support z-function z-rust">learning_strategy</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span><span class="z-meta z-path z-rust">burn<span class="z-punctuation z-accessor z-rust">::</span></span><span class="z-meta z-path z-rust">train<span class="z-punctuation z-accessor z-rust">::</span></span>ddp<span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span><span class="z-support z-macro z-rust">vec!</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">[</span>gpu_1<span class="z-punctuation z-separator z-rust">,</span> gpu_2<span class="z-punctuation z-separator z-rust">,</span> gpu_3<span class="z-punctuation z-section z-group z-end z-rust">]</span></span><span class="z-punctuation z-separator z-rust">,</span> collective</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span>
</span><span class="z-source z-rust">    <span class="z-comment z-line z-double-slash z-rust"><span class="z-punctuation z-definition z-comment z-rust">//</span> ...
</span></span><span class="z-source z-rust">    <span class="z-punctuation z-accessor z-dot z-rust">.</span><span class="z-support z-function z-rust">build</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span>model<span class="z-punctuation z-separator z-rust">,</span> config<span class="z-punctuation z-accessor z-dot z-rust">.</span>optimizer<span class="z-punctuation z-accessor z-dot z-rust">.</span><span class="z-support z-function z-rust">init</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span><span class="z-punctuation z-separator z-rust">,</span> 1e<span class="z-keyword z-operator z-arithmetic z-rust">-</span><span class="z-constant z-numeric z-integer z-decimal z-rust">4</span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span><span class="z-punctuation z-terminator z-rust">;</span>
</span></code></pre><p>The DDP learning strategy will launch a thread for each device, so in single-node environments this is a minimal change.<p>For multi-node environments, the user will need to launch the <code>GlobalOrchestrator</code>. After, they will need to launch the training on each node manually. Extra configuration is also required for the nodes to find each other.<pre class="language-rust z-code" data-lang=rust><code class=language-rust data-lang=rust><span class="z-source z-rust"><span class="z-storage z-type z-rust">let</span> collective <span class="z-keyword z-operator z-assignment z-rust">=</span> <span class="z-meta z-path z-rust">CollectiveConfig<span class="z-punctuation z-accessor z-rust">::</span></span>default<span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span>
</span><span class="z-source z-rust">    <span class="z-punctuation z-accessor z-dot z-rust">.</span><span class="z-support z-function z-rust">with_global_address</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span><span class="z-meta z-path z-rust">Address<span class="z-punctuation z-accessor z-rust">::</span></span>from_str<span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span><span class="z-string z-quoted z-double z-rust"><span class="z-punctuation z-definition z-string z-begin z-rust">"</span>ws://example.com/orchestrator<span class="z-punctuation z-definition z-string z-end z-rust">"</span></span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span><span class="z-punctuation z-accessor z-dot z-rust">.</span><span class="z-support z-function z-rust">unwrap</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span>
</span><span class="z-source z-rust">    <span class="z-punctuation z-accessor z-dot z-rust">.</span><span class="z-support z-function z-rust">with_num_nodes</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span><span class="z-constant z-numeric z-integer z-decimal z-rust">3</span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span>
</span><span class="z-source z-rust">    <span class="z-punctuation z-accessor z-dot z-rust">.</span><span class="z-support z-function z-rust">with_node_address</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span><span class="z-meta z-path z-rust">Address<span class="z-punctuation z-accessor z-rust">::</span></span>from_str<span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span><span class="z-string z-quoted z-double z-rust"><span class="z-punctuation z-definition z-string z-begin z-rust">"</span>ws://example.com/node_1<span class="z-punctuation z-definition z-string z-end z-rust">"</span></span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span><span class="z-punctuation z-accessor z-dot z-rust">.</span><span class="z-support z-function z-rust">unwrap</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span>
</span><span class="z-source z-rust">    <span class="z-punctuation z-accessor z-dot z-rust">.</span><span class="z-support z-function z-rust">with_data_service_port</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span><span class="z-constant z-numeric z-integer z-decimal z-rust">3000</span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span><span class="z-punctuation z-terminator z-rust">;</span>
</span><span class="z-source z-rust">
</span><span class="z-source z-rust"><span class="z-storage z-type z-rust">let</span> learner <span class="z-keyword z-operator z-assignment z-rust">=</span> <span class="z-meta z-path z-rust">LearnerBuilder<span class="z-punctuation z-accessor z-rust">::</span></span>new<span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span><span class="z-constant z-other z-rust">ARTIFACT_DIR</span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span>
</span><span class="z-source z-rust">    <span class="z-punctuation z-accessor z-dot z-rust">.</span><span class="z-support z-function z-rust">learning_strategy</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span><span class="z-meta z-path z-rust">burn<span class="z-punctuation z-accessor z-rust">::</span></span><span class="z-meta z-path z-rust">train<span class="z-punctuation z-accessor z-rust">::</span></span>ddp<span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span><span class="z-support z-macro z-rust">vec!</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">[</span>gpu_1<span class="z-punctuation z-separator z-rust">,</span> gpu_2<span class="z-punctuation z-separator z-rust">,</span> gpu_3<span class="z-punctuation z-section z-group z-end z-rust">]</span></span><span class="z-punctuation z-separator z-rust">,</span> collective</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span>
</span><span class="z-source z-rust">    <span class="z-comment z-line z-double-slash z-rust"><span class="z-punctuation z-definition z-comment z-rust">//</span> ...
</span></span><span class="z-source z-rust">    <span class="z-punctuation z-accessor z-dot z-rust">.</span><span class="z-support z-function z-rust">build</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span>model<span class="z-punctuation z-separator z-rust">,</span> config<span class="z-punctuation z-accessor z-dot z-rust">.</span>optimizer<span class="z-punctuation z-accessor z-dot z-rust">.</span><span class="z-support z-function z-rust">init</span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-begin z-rust">(</span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span><span class="z-punctuation z-separator z-rust">,</span> 1e<span class="z-keyword z-operator z-arithmetic z-rust">-</span><span class="z-constant z-numeric z-integer z-decimal z-rust">4</span></span><span class="z-meta z-group z-rust"><span class="z-punctuation z-section z-group z-end z-rust">)</span></span><span class="z-punctuation z-terminator z-rust">;</span>
</span></code></pre><h1 id=conclusion><a aria-label="Anchor link for: conclusion" class=zola-anchor href=#conclusion>Conclusion</a></h1><p>With burn-collective and the new DDP learning strategy, training on multiple GPUs or even across multiple nodes is now straightforward in Burn. On a single machine, users only need to provide their devices—the framework handles threading and gradient synchronization automatically. Scaling to multiple nodes requires some extra configuration for the orchestrator and addresses, but the API stays consistent, and the communication layer abstracts away the complexity.<p>The key point is that you don’t need to learn NCCL, MPI, or low-level communication details. Burn provides a unified interface for collective operations that works across devices and nodes, while still letting you choose strategies that match your hardware. This makes it possible to start small and scale up without rewriting your training code.<p>If you’re already training models in Burn, upgrading to distributed data parallel training is just a few lines of code away.</section></article></main></div><div class=giscus></div><script async crossorigin issue-term=pathname repo=YOUR_NAME/YOUR_REPO src=https://utteranc.es/client.js theme=github-light></script></div><div class=right-content><div class=toc><div class=heading>Table of Contents</div><ul class=toc-list><li class=parent><a href=https://cielbird.github.io/blog/burn-distributed/#introduction>Introduction</a><li class=parent><a href=https://cielbird.github.io/blog/burn-distributed/#data-parallel-training>Data Parallel Training</a><li class=parent><a href=https://cielbird.github.io/blog/burn-distributed/#collective-operations>Collective operations</a><li class=parent><a href=https://cielbird.github.io/blog/burn-distributed/#how-burn-implements-collective-operations>How Burn implements collective operations</a> <ul><li><a href=https://cielbird.github.io/blog/burn-distributed/#how-many-processes>How many processes?</a><li><a href=https://cielbird.github.io/blog/burn-distributed/#local-and-global>Local and global</a><li><a href=https://cielbird.github.io/blog/burn-distributed/#walkthrough-of-an-all-reduce>Walkthrough of an All-Reduce</a><li><a href=https://cielbird.github.io/blog/burn-distributed/#methods>Methods</a></li><ul><li><a href=https://cielbird.github.io/blog/burn-distributed/#centralized>Centralized</a><li><a href=https://cielbird.github.io/blog/burn-distributed/#tree>Tree</a><li><a href=https://cielbird.github.io/blog/burn-distributed/#ring>Ring</a><li><a href=https://cielbird.github.io/blog/burn-distributed/#local-strategy-and-global-strategy>Local strategy and global strategy</a><li><a href=https://cielbird.github.io/blog/burn-distributed/#local-ring-downfall>Local ring downfall</a></ul></ul><li class=parent><a href=https://cielbird.github.io/blog/burn-distributed/#burn-communications>burn-communications</a><li class=parent><a href=https://cielbird.github.io/blog/burn-distributed/#ddp-training>DDP Training</a><li class=parent><a href=https://cielbird.github.io/blog/burn-distributed/#conclusion>Conclusion</a></ul></div></div>